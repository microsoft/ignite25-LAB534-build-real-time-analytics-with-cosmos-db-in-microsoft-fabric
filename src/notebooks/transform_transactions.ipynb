{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae1245b7-5646-4ac6-84e1-8559e29431f6",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Transform and Load Eventhouse Data\n",
    "\n",
    "This notebook transforms streaming POS transactions from Cosmos DB in Fabric into clean, conformed facts for the Fourth Coffee Gold Layer. \n",
    "It reads Silver layer data from KQL functions, joins with warehouse dimensions to assign stage keys, stages Parquet files with normalized types that are used to run `COPY INTO` commands in the data warehouse to rapidly load the FactSales and FactSalesLineItem tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8078662b-2d28-423f-b3cb-50f53ef25e6e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "from pyspark.sql.types import DecimalType\n",
    "\n",
    "dec2 = DecimalType(10,2)\n",
    "\n",
    "\n",
    "\n",
    "dw_fact_sales          = \"dbo.FactSales\"\n",
    "dw_fact_sales_line     = \"dbo.FactSalesLineItems\"\n",
    "dw_v_dim_customer_key  = \"dbo.vDimCustomerKey\"\n",
    "dw_v_dim_shop_key      = \"dbo.vDimShopKey\"\n",
    "dw_v_dim_menu_key      = \"dbo.vDimMenuItemKey\"\n",
    "dw_v_fact_max          = \"dbo.vFactSalesMaxKey\"\n",
    "\n",
    "kustoCluster = \"\" # Find in fc_commerce_eventhouse > Copy Query URI in Eventhouse Details on right or in textfile\n",
    "kustoDatabase = \"fc_commerce_eventhouse\"\n",
    "pos_sales = \"vw_Pos_Sales()\"\n",
    "pos_line_items = \"vw_Pos_LineItems_Sales()\"\n",
    "\n",
    "workspace_guid = \"\" # Copy first guid in browser address (eg https://app.fabric.microsoft.com/groups/[your workspace guid]/)\n",
    "lakehouse_guid= \"\" # Open lakehouse from your workspace and copy second guid in the browser address (eg https://app.fabric.microsoft.com/groups/[your workspace guid]/lakehouses/[lakehouse guid])\n",
    "\n",
    "WAREHOUSE = \"fc_commerce_warehouse\"  \n",
    "SERVER    = \"\"  # go to warehouse > settings > copy sql endpoint (e.g., \"abcd1234-...-workspace.z01.datawarehouse.fabric.microsoft.com\")\n",
    "\n",
    "# 1) Get an Entra ID token from the Fabric runtime \n",
    "from notebookutils.mssparkutils import credentials\n",
    "token = credentials.getToken(\"pbi\")  # ~60–90 min lifetime\n",
    "# 2) Build JDBC URL & properties\n",
    "jdbc_url = (\n",
    "    f\"jdbc:sqlserver://{SERVER}:1433;\"\n",
    "    f\"database={WAREHOUSE};\"\n",
    "    \"encrypt=true;\"\n",
    "    \"trustServerCertificate=false;\"\n",
    "    \"hostNameInCertificate=*.datawarehouse.fabric.microsoft.com;\"\n",
    "    \"loginTimeout=30\"\n",
    ")\n",
    "props = {\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "    \"accessToken\": token\n",
    "}\n",
    "\n",
    "accessToken = mssparkutils.credentials.getToken(kustoCluster)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3dc53f-87a2-4e59-9e3d-09b77685d6af",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Create Convenience Helpers and Read streaming data from KQL (Silver layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963abb70-e6b9-4549-971c-f7a2ff33317b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Convenience helpers\n",
    "def read_kql_function(func: str):\n",
    "    # Kusto (KQL) Spark connector\n",
    "    return (spark.read\\\n",
    "    .format(\"com.microsoft.kusto.spark.synapse.datasource\")\\\n",
    "    .option(\"accessToken\", accessToken)\\\n",
    "    .option(\"kustoCluster\", kustoCluster)\\\n",
    "    .option(\"kustoDatabase\", kustoDatabase)\\\n",
    "    .option(\"kustoQuery\", func).load())\n",
    "\n",
    "\n",
    "hdr_raw  = read_kql_function(pos_sales)   # TransactionId, DateKey, TimeKey, CustomerId, ShopId, TotalQuantity, TotalAmount, PaymentMethod, LoyaltyPoints*, CreatedAt\n",
    "line_raw = read_kql_function(pos_line_items)    # TransactionId, LineNumber, DateKey, TimeKey, (CustomerId, ShopId optional), MenuItemId/Key, Quantity, UnitPrice, LineTotal, PaymentMethod, Size, CreatedAt\n",
    "\n",
    "print(hdr_raw.count(), \"header rows (silver)\")\n",
    "print(line_raw.count(), \"line rows (silver)\")\n",
    "\n",
    "def read_dw_table(table: str):\n",
    "    return spark.read.jdbc(jdbc_url, table=table, properties=props)\n",
    "\n",
    "print(read_dw_table(dw_fact_sales))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7d354f-0bec-42d3-9975-6734b0f1e5af",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "v_cust  = read_dw_table(dw_v_dim_customer_key)  # CustomerId, CustomerKey, (IsActive)\n",
    "v_shop  = read_dw_table(dw_v_dim_shop_key)      # ShopId, ShopKey, (IsActive)\n",
    "v_menu  = read_dw_table(dw_v_dim_menu_key)      # MenuItemId, MenuItemKey, (IsActive)\n",
    "v_max   = read_dw_table(dw_v_fact_max)          # MaxSalesKey, ExistingTxnCount\n",
    "\n",
    "max_sales_key = v_max.select(F.coalesce(F.col(\"MaxSalesKey\"), F.lit(0)).alias(\"MaxSalesKey\")).first()[\"MaxSalesKey\"]\n",
    "print(\"MaxSalesKey in DW =\", max_sales_key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec19ba0-1f11-4271-80eb-de316620e686",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Resolve business → surrogate keys (dimension lookups) and Assign new SalesKey values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953e11af-2f1d-407b-bc75-17254d15b1e1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Optionally read existing TransactionIds to enforce idempotency\n",
    "existing_txn_df = (read_dw_table(dw_fact_sales)\n",
    "                   .select(\"TransactionId\")\n",
    "                   .dropDuplicates())\n",
    "\n",
    "# Left-anti to keep only brand-new TransactionIds\n",
    "hdr_new = (hdr_raw\n",
    "           .join(existing_txn_df, on=\"TransactionId\", how=\"left_anti\"))\n",
    "\n",
    "# Resolve CustomerKey & ShopKey\n",
    "hdr_keys = (hdr_new\n",
    "  .join(v_cust.select(\"CustomerId\",\"CustomerKey\"), on=\"CustomerId\", how=\"left\")\n",
    "  .join(v_shop.select(\"ShopId\",\"ShopKey\"), on=\"ShopId\", how=\"left\"))\n",
    "\n",
    "# Assign SalesKey = MaxSalesKey + row_number()\n",
    "window_tx = Window.orderBy(F.col(\"TransactionId\"))\n",
    "hdr_final = (hdr_keys\n",
    "  .withColumn(\"rn\", F.row_number().over(window_tx))\n",
    "  .withColumn(\"SalesKey\", F.col(\"rn\") + F.lit(int(max_sales_key)))\n",
    "  .drop(\"rn\"))\n",
    "\n",
    "# Reorder/select to match FactSales schema\n",
    "hdr_out = (hdr_final.select(\n",
    "    \"SalesKey\",\n",
    "    \"TransactionId\",\"DateKey\",\"TimeKey\",\n",
    "    \"CustomerKey\",\"ShopKey\",\n",
    "    \"TotalQuantity\",\"TotalAmount\",\n",
    "    \"PaymentMethod\",\n",
    "    \"LoyaltyPointsEarned\",\"LoyaltyPointsRedeemed\",\n",
    "    \"CreatedAt\"\n",
    "))\n",
    "\n",
    "tx_to_key = hdr_out.select(\"TransactionId\", \"SalesKey\").dropDuplicates()\n",
    "\n",
    "hdr_out.printSchema()\n",
    "print(hdr_out.count(), \"new header rows to insert\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85dd8d7-a37d-42db-8bb3-359ac6e6b488",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Normalize item data and handle mixed keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb43d70-e5d8-4ff5-9f42-dac296e76482",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# --- Normalize the line DataFrame (handle both MenuItemId/menuItemId and keep incoming MenuItemKey too) ---\n",
    "line_src = (line_raw\n",
    "    .withColumn(\"LineMenuItemId\", F.coalesce(F.col(\"MenuItemId\"), F.col(\"menuItemId\")))\n",
    "    .select(\n",
    "        \"TransactionId\",\"LineNumber\",\n",
    "        \"DateKey\",\"TimeKey\",\n",
    "        \"MenuItemKey\",          # incoming key (may be null or wrong; we'll coalesce to DW)\n",
    "        \"LineMenuItemId\",\n",
    "        \"Quantity\",\"UnitPrice\",\"LineTotal\",\n",
    "        \"PaymentMethod\",\"Size\",\"CreatedAt\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# --- Prepare the DW menu lookup with clear aliases and dedup on business key ---\n",
    "v_menu_sel = (v_menu\n",
    "    .withColumn(\"DimMenuItemId\", F.coalesce(F.col(\"MenuItemId\"), F.col(\"menuItemId\")))\n",
    "    .select(\"DimMenuItemId\", \"MenuItemKey\")\n",
    "    .dropDuplicates([\"DimMenuItemId\"])\n",
    ")\n",
    "\n",
    "# --- Join (DW as source of truth), then coalesce DW key -> incoming key ---\n",
    "line_joined = (line_src.alias(\"l\")\n",
    "    .join(v_menu_sel.alias(\"m\"),\n",
    "          F.col(\"l.LineMenuItemId\") == F.col(\"m.DimMenuItemId\"),\n",
    "          \"left\")\n",
    "    .withColumn(\"MenuItemKeyFinal\",\n",
    "                F.coalesce(F.col(\"m.MenuItemKey\"), F.col(\"l.MenuItemKey\")))\n",
    ")\n",
    "\n",
    "# --- Keep only the line items for transactions that will be inserted in this run ---\n",
    "tx_new_ids = hdr_out.select(\"TransactionId\").dropDuplicates()\n",
    "line_joined_new = line_joined.join(tx_new_ids, on=\"TransactionId\", how=\"inner\")\n",
    "\n",
    "# Bring in SalesKey from header map with clear aliases\n",
    "line_new = (line_joined_new.alias(\"l\")\n",
    "    .join(tx_to_key.alias(\"k\"),\n",
    "          F.col(\"l.TransactionId\") == F.col(\"k.TransactionId\"),\n",
    "          \"left\")\n",
    ")\n",
    "\n",
    "# Final column order — qualify every column to avoid ambiguity,\n",
    "# and use the DW-normalized key you created (MenuItemKeyFinal)\n",
    "line_out = (line_new.select(\n",
    "    F.col(\"l.TransactionId\").alias(\"TransactionId\"),\n",
    "    F.col(\"k.SalesKey\").alias(\"SalesKey\"),\n",
    "    F.col(\"l.LineNumber\").alias(\"LineNumber\"),\n",
    "    F.col(\"l.DateKey\").alias(\"DateKey\"),\n",
    "    F.col(\"l.TimeKey\").alias(\"TimeKey\"),\n",
    "    F.col(\"l.MenuItemKeyFinal\").alias(\"MenuItemKey\"),\n",
    "    F.col(\"l.Quantity\").alias(\"Quantity\"),\n",
    "    F.col(\"l.UnitPrice\").alias(\"UnitPrice\"),\n",
    "    F.col(\"l.LineTotal\").alias(\"LineTotal\"),\n",
    "    F.col(\"l.PaymentMethod\").alias(\"PaymentMethod\"),\n",
    "    F.col(\"l.Size\").alias(\"Size\"),\n",
    "    F.col(\"l.CreatedAt\").alias(\"CreatedAt\")\n",
    "))\n",
    "\n",
    "line_out.printSchema()\n",
    "print(line_out.count(), \"new line rows to insert\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b508a8-01b7-42aa-b53d-00a5b886250b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Stage Gold data to OneLake as Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3254b9a-03e3-43a9-b03d-b957f32c7a1e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "hdr_stage_path  = f\"Files/factsales/\"\n",
    "line_stage_path = f\"Files/factsales_lineitems/\"\n",
    "\n",
    "\n",
    "hdr_stage = (hdr_out\n",
    "  .withColumn(\"TotalAmount\", F.round(\"TotalAmount\", 2).cast(dec2))\n",
    "  .withColumn(\"LoyaltyPointsEarned\",  F.col(\"LoyaltyPointsEarned\").cast(\"int\"))\n",
    "  .withColumn(\"LoyaltyPointsRedeemed\",F.col(\"LoyaltyPointsRedeemed\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "line_stage = (line_out\n",
    "  .withColumn(\"UnitPrice\", F.round(\"UnitPrice\", 2).cast(dec2))\n",
    "  .withColumn(\"LineTotal\", F.round(\"LineTotal\", 2).cast(dec2))\n",
    "  .withColumn(\"Quantity\",  F.col(\"Quantity\").cast(\"int\"))\n",
    ")\n",
    "\n",
    "\n",
    "# Coalesce to 1–2 files; Parquet is fastest for COPY\n",
    "# WARNING: This will overwrite any data at these target locations!\n",
    "(hdr_stage.coalesce(1)\n",
    "    .write.mode(\"overwrite\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .parquet(hdr_stage_path))\n",
    "\n",
    "(line_stage.coalesce(1)\n",
    "    .write.mode(\"overwrite\")\n",
    "    .option(\"compression\", \"snappy\")\n",
    "    .parquet(line_stage_path))\n",
    "\n",
    "print(\"Staged to OneLake:\")\n",
    "print(\"  \", hdr_stage_path)\n",
    "print(\"  \", line_stage_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd11f85-3cec-46be-9dda-4ec6779204b0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Copy SQL Script and lakehouse urls to load data into Data Warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1bcf3c-40cd-4101-989a-3cf069113304",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "```sql\n",
    "-- =============================\n",
    "-- COPY headers (FactSales)\n",
    "-- =============================\n",
    "COPY INTO dbo.FactSales\n",
    "(\n",
    "    SalesKey             1,\n",
    "    TransactionId        2,\n",
    "    DateKey              3,\n",
    "    TimeKey              4,\n",
    "    CustomerKey          5,\n",
    "    ShopKey              6,\n",
    "    TotalQuantity        7,\n",
    "    TotalAmount          8,\n",
    "    PaymentMethod        9,\n",
    "    LoyaltyPointsEarned 10,\n",
    "    LoyaltyPointsRedeemed 11,\n",
    "    CreatedAt           12\n",
    ")\n",
    "FROM '[lakehouse sales url]' \n",
    "WITH (\n",
    "    FILE_TYPE = 'PARQUET'\n",
    ");\n",
    "\n",
    "\n",
    "\n",
    "-- =============================\n",
    "-- COPY lines (FactSalesLineItems)\n",
    "-- =============================\n",
    "COPY INTO dbo.FactSalesLineItems\n",
    "(\n",
    "    TransactionId  1,\n",
    "    SalesKey       2,\n",
    "    LineNumber     3,\n",
    "    DateKey        4,\n",
    "    TimeKey        5,\n",
    "    MenuItemKey    6,\n",
    "    Quantity       7,\n",
    "    UnitPrice      8,\n",
    "    LineTotal      9,\n",
    "    PaymentMethod 10,\n",
    "    Size          11,\n",
    "    CreatedAt     12\n",
    ")\n",
    "FROM '[lakehouse items urls]' \n",
    "WITH (\n",
    "    FILE_TYPE = 'PARQUET'\n",
    ");\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3976369a-8fba-4ba7-a378-6d948bf06f3b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "lakehouse_url_sales = f\"https://onelake.dfs.fabric.microsoft.com/{workspace_guid}/{lakehouse_guid}/{hdr_stage_path}\"\n",
    "lakehouse_url_items = f\"https://onelake.dfs.fabric.microsoft.com/{workspace_guid}/{lakehouse_guid}/{line_stage_path}\"\n",
    "\n",
    "print(\"Your OneLake URL for FactSales (copy for COPY INTO, etc):\\n\", lakehouse_url_sales)\n",
    "print(\"Your OneLake URL for FactSalesLineItems (copy for COPY INTO, etc):\\n\", lakehouse_url_items)"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
