{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "263b1193-87a0-4d0f-a4b3-42f8324e64b4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Reverse ETL and Personalization Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af0e313-5655-4383-8e20-523b53f803ea",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#Install packages\n",
    "%pip install azure-cosmos\n",
    "%pip install azure-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075829f-d4d0-4df1-ba20-14ceed7df509",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "#Imports and config values\n",
    "import base64, json\n",
    "from typing import Any, Optional\n",
    "\n",
    "#from azure.cosmos.aio import CosmosClient why aio\n",
    "from azure.cosmos import CosmosClient, PartitionKey, ThroughputProperties\n",
    "from azure.core.credentials import TokenCredential, AccessToken\n",
    "\n",
    "\n",
    "COSMOS_ENDPOINT = '' # The Cosmos DB endpoint, found in Settings (gear icon) > Connection\n",
    "COSMOS_DATABASE_NAME = 'fc_commerce_cosmos' \n",
    "COSMOS_CONTAINER_NAME = 'customers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff89755-6811-4815-a937-c37bcbeccd01",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "class FabricTokenCredential(TokenCredential):\n",
    "    \"\"\"Token credential for Fabric Cosmos DB access with automatic refresh and retry logic.\"\"\"\n",
    "    \n",
    "    def get_token(self, *scopes: str, claims: Optional[str] = None, tenant_id: Optional[str] = None,\n",
    "                  enable_cae: bool = False, **kwargs: Any) -> AccessToken:\n",
    "        access_token = notebookutils.credentials.getToken(\"https://cosmos.azure.com/.default\")\n",
    "        parts = access_token.split(\".\")\n",
    "        if len(parts) < 2:\n",
    "            raise ValueError(\"Invalid JWT format\")\n",
    "        payload_b64 = parts[1]\n",
    "        # Fix padding\n",
    "        padding = (-len(payload_b64)) % 4\n",
    "        if padding:\n",
    "            payload_b64 += \"=\" * padding\n",
    "        payload_json = base64.urlsafe_b64decode(payload_b64.encode(\"utf-8\")).decode(\"utf-8\")\n",
    "        payload = json.loads(payload_json)\n",
    "        exp = payload.get(\"exp\")\n",
    "        if exp is None:\n",
    "            raise ValueError(\"exp claim missing in token\")\n",
    "        return AccessToken(token=access_token, expires_on=exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1142a2-d45a-426e-90d8-d617f2775b4b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize Cosmos DB cosmos client\n",
    "COSMOS_CLIENT = CosmosClient(COSMOS_ENDPOINT, FabricTokenCredential())\n",
    "\n",
    "# Initialize Cosmos DB database client\n",
    "DATABASE_CLIENT = COSMOS_CLIENT.get_database_client(COSMOS_DATABASE_NAME)\n",
    "\n",
    "# Intialize Cosmos DB container client\n",
    "CONTAINER_CLIENT = DATABASE_CLIENT.get_container_client(COSMOS_CONTAINER_NAME) # Default is SampleData\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff9e8e-ec1c-4c63-aabd-c290e5159777",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "queryText = \"\"\"SELECT c.customerId,\n",
    "        (SELECT VALUE COUNT(1) FROM r IN c.recommendations) AS recommendationSets,\n",
    "        (SELECT VALUE COUNT(1) FROM r IN c.recommendations JOIN mi IN r.menuItems) AS totalRecommendedItems,\n",
    "        (SELECT VALUE ROUND(AVG(r.score), 4) FROM r IN c.recommendations) AS avgRecScore,\n",
    "        (SELECT VALUE MIN(r.expiresAt) FROM r IN c.recommendations) AS nextExpiryUtc\n",
    "       FROM customers c\"\"\"\n",
    "\n",
    "results = CONTAINER_CLIENT.query_items(\n",
    "    query=queryText,\n",
    "    parameters=[\n",
    "    ],\n",
    "    enable_cross_partition_query=True,\n",
    ")\n",
    "\n",
    "results_list = list(results)\n",
    "df = spark.createDataFrame(results_list)\n",
    "# pd.DataFrame(results_list)\n",
    "\n",
    "print(\"Retrieved\", df.count(), \"records from Cosmos.\")\n",
    "# df.head()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8b490b-f147-4430-a3bf-ff3f39b19805",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "WAREHOUSE = \"fc_commerce_warehouse\"  \n",
    "SERVER    = \"\" # go to warehouse > settings > copy sql endpoint (e.g., \"abcd1234-...-workspace.z01.datawarehouse.fabric.microsoft.com\")\n",
    "\n",
    "# 1) Get an Entra ID token from the Fabric runtime \n",
    "from notebookutils.mssparkutils import credentials\n",
    "token = credentials.getToken(\"pbi\")  # ~60–90 min lifetime\n",
    "# 2) Build JDBC URL & properties\n",
    "jdbc_url = (\n",
    "    f\"jdbc:sqlserver://{SERVER}:1433;\"\n",
    "    f\"database={WAREHOUSE};\"\n",
    "    \"encrypt=true;\"\n",
    "    \"trustServerCertificate=false;\"\n",
    "    \"hostNameInCertificate=*.datawarehouse.fabric.microsoft.com;\"\n",
    "    \"loginTimeout=30\"\n",
    ")\n",
    "props = {\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "    \"accessToken\": token\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "dw_fact_sales = \"dbo.FactSales\"\n",
    "dw_fact_sales_line = \"dbo.FactSalesLineItems\"\n",
    "dw_dim_customer = \"dbo.DimCustomer\"\n",
    "\n",
    "def read_dw_table(table: str):\n",
    "    return spark.read.jdbc(jdbc_url, table=table, properties=props)\n",
    "\n",
    "print(read_dw_table(dw_fact_sales))\n",
    "\n",
    "fact_sales  = read_dw_table(dw_fact_sales)  \n",
    "fact_line_items  = read_dw_table(dw_fact_sales_line)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00740c1-7fde-4a63-ae27-2a63cb12f959",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Aggregate loyalty deltas from FactSales\n",
    "loyalty_agg = (fact_sales\n",
    "    .groupBy(\"CustomerKey\")\n",
    "    .agg(\n",
    "        F.coalesce(F.sum(\"LoyaltyPointsEarned\"), F.lit(0)).alias(\"PointsEarned\"),\n",
    "        F.coalesce(F.sum(\"LoyaltyPointsRedeemed\"), F.lit(0)).alias(\"PointsRedeemed\"),\n",
    "        F.max(\"CreatedAt\").alias(\"LastPurchaseUtc\")\n",
    "    )\n",
    "    .withColumn(\"NewLoyaltyPoints\", F.col(\"PointsEarned\") - F.col(\"PointsRedeemed\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bdc436-47c8-4777-975a-02ae8362c0bd",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "queryText = \"select * from c\"\n",
    "\n",
    "results = CONTAINER_CLIENT.query_items(\n",
    "    query=queryText,\n",
    "    parameters=[\n",
    "    ],\n",
    "    enable_cross_partition_query=True,\n",
    ")\n",
    "\n",
    "results_list = list(results)\n",
    "customers_df = spark.createDataFrame(results_list)\n",
    "\n",
    "print(\"Retrieved\", customers_df.count(), \"records from Cosmos.\")\n",
    "customers_df.select(\"customerId\",\"name\",\"loyaltyPoints\",\"lastPurchaseDate\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909d2284-f18a-4743-9f16-8cd30ad56ee3",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Bring CustomerId to join with Cosmos (via DimCustomer)\n",
    "dim_customer = read_dw_table(dw_dim_customer).select(\"CustomerKey\",\"CustomerId\")\n",
    "\n",
    "loyalty_by_id = (loyalty_agg\n",
    "    .join(dim_customer, on=\"CustomerKey\", how=\"inner\")\n",
    "    .select(\"CustomerId\",\"NewLoyaltyPoints\",\"LastPurchaseUtc\")\n",
    ")\n",
    "\n",
    "# Join with existing customer docs and produce upserts\n",
    "# Keep all fields in the doc, only replace loyaltyPoints / lastPurchaseDate / updatedAt\n",
    "cust_with_loyalty = (customers_df.alias(\"c\")\n",
    "    .join(loyalty_by_id.alias(\"l\"), F.col(\"c.customerId\") == F.col(\"l.CustomerId\"), \"left\")\n",
    "    .withColumn(\"loyaltyPoints\", F.when(F.col(\"l.NewLoyaltyPoints\").isNotNull(), F.col(\"l.NewLoyaltyPoints\")).otherwise(F.col(\"c.loyaltyPoints\")))\n",
    "    .withColumn(\"lastPurchaseDate\", F.when(F.col(\"l.LastPurchaseUtc\").isNotNull(), F.col(\"l.LastPurchaseUtc\").cast(\"timestamp\")).otherwise(F.col(\"c.lastPurchaseDate\")))\n",
    "    .withColumn(\"updatedAt\", F.current_timestamp())\n",
    "    .drop(\"NewLoyaltyPoints\",\"LastPurchaseUtc\",\"CustomerKey\")\n",
    ")\n",
    "\n",
    "display(cust_with_loyalty.limit(10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252d145-4987-4279-852d-aad15a0416fc",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Upsert back to Cosmos\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 1) Filter to \"purchased today\" in UTC\n",
    "today_updates = (loyalty_by_id\n",
    "    .filter(F.to_date(\"LastPurchaseUtc\") == F.current_date())\n",
    "    .select(\"CustomerId\", \"NewLoyaltyPoints\", \"LastPurchaseUtc\")\n",
    ")\n",
    "\n",
    "print(\"Customers with purchases today:\", today_updates.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57b40d2-a4d8-403e-9db3-ee0e8433f7f2",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# 2) Patch each customer document in Cosmos\n",
    "#    Adjust partition_key below to match your container (e.g., /customerId or /id).\n",
    "patched, failed = 0, 0\n",
    "\n",
    "for row in today_updates.toLocalIterator():\n",
    "    cust_id  = row[\"CustomerId\"]\n",
    "    points   = int(row[\"NewLoyaltyPoints\"] or 0)\n",
    "    last_dt  = row[\"LastPurchaseUtc\"]              # Py datetime (UTC) from Spark\n",
    "    last_iso = last_dt.isoformat().replace(\"+00:00\",\"Z\") if last_dt.tzinfo else last_dt.isoformat() + \"Z\"\n",
    "\n",
    "    ops = [\n",
    "        {\"op\":\"set\", \"path\":\"/loyaltyPoints\", \"value\":points},\n",
    "        {\"op\":\"set\", \"path\":\"/lastPurchaseDate\", \"value\":last_iso},\n",
    "        {\"op\":\"set\", \"path\":\"/updatedAt\", \"value\":datetime.utcnow().isoformat() + \"Z\"}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        CONTAINER_CLIENT.patch_item(item=cust_id, partition_key=cust_id, patch_operations=ops)\n",
    "        patched += 1\n",
    "    except Exception as e:\n",
    "        failed += 1\n",
    "        print(f\"⚠️ Patch failed for {cust_id}: {e}\")\n",
    "\n",
    "print(f\"✅ Patched: {patched} | ❌ Failed: {failed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8018892e-7ae2-462c-81ea-e11ad0ced2a3",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Read back a few updated customers\n",
    "check_query = \"\"\"\n",
    "SELECT TOP 10 c.customerId, c.name, c.loyaltyPoints, c.lastPurchaseDate, c.updatedAt\n",
    "FROM c\n",
    "WHERE IS_DEFINED(c.updatedAt)\n",
    "ORDER BY c.updatedAt DESC\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    results_iter = CONTAINER_CLIENT.query_items(\n",
    "        query=check_query,\n",
    "        enable_cross_partition_query=True\n",
    "    )\n",
    "    results_list = list(results_iter)\n",
    "    df_check = spark.createDataFrame(results_list)\n",
    "    print(f\"✅ Retrieved {df_check.count()} recently updated customers\")\n",
    "    display(df_check)  # Fabric notebook pretty table\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Query failed:\", e)\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {},
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
